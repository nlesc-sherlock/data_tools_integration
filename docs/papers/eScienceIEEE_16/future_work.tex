\section{Future work}
\label{future_work}

Integration of data vaults into Spark.

Access only to Hive Tables.

Monunt external data files as Data Frames.
A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.

In Spark not possible to make and RDD out of NetCDF.
There are specialized modules for each tool, but the combination of different data sets is not possible.


http://stackoverflow.com/questions/11816609/column-based-or-row-based-for-hbase


http://www.sparkexpert.com/2015/01/02/load-database-data-into-spark-using-jdbcrdd-in-java/

In Spark 1.2, weâ€™ve taken the next step to allow Spark to integrate natively with a far larger number of input sources.  These new integrations are made possible through the inclusion of the new Spark SQL Data Sources API.

https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html
