\section{Future work}
\label{future_work}

----Towards Generating ETL Processes for Incremental Loading---
We presented an approach for generating incremental load processes for data  warehouse  refreshment  from
declarative schema mappings.  

The basis of our work has been provided by Orchid, a prototype system developed at IBM Almaden Research
Center, that translates schema mappings into ETL processes and vice versa.  Orchid-generated ETL processes,
however, are limited to initial load scenarios, i.e. source data is exhaustively extracted and the warehouse
is completely (re)built. Incremental load processes, in contrast, propagate changes from the sources to the
data warehouse. This approach has clear performance benefits. Change Data Capture (CDC) and Change Data
Application (CDA) techniques are used to capture changes at the sources and refresh the data warehouse,
respectively.  We defined a model for change data to  characterize  both,  the  output  of  CDC techniques 
and  the  input  of  CDA  techniques.   Since  CDC techniques may su from limitations we introduced a notion
of partial  change  data.  We discussed the propagation of  partial  change  data  within  incremental  load
processes.

Our  approach  allows  for  reasoning  on  how  limitations  of CDC techniques determine the set of applicable
CDA techniques.  That is, it allows inferring satisticable CDA reuirements from given CDC limitations and, the
other way round, acceptable CDC limitations from given CDA requirements. We further, demonstrated the exploitation
of properties of data sources (such as schema constraints) to reduce the complexity  of  incremental  load
processes.

\subsection{Stepping way from HDFS}
Outside HDFS cluster we use keubernet to schedule docker containers over each node. A spark cluster can be
created, but also a set of containers running othere tools. Using messos for resource management, several
processing tools can co-exist at the same time.

The challenge becomes to access heterogenous data and find a common ground to share data. The idea is to
exploit data vaults when using a DBMS or Spark. Specialized libraries work with a single data format. They
are used often in data preparation phase where classificaiton happens. 

Data vaults is not only about importing data, it is also about exporting data. We are inverting the paradigm
with this platform, it is not a DBMS on top of Spark or Hadoop, but both co-exist side by side where the query
plan abstraction goes on level up. With this query interaction is more user friendly, instead of gathering
information we are now extracting knowledge.

Using Kubernet it is the first step towards running Spark with access to external file repositories. For that
we will use and extend the latest API released: %link missing.
In Spark 1.2, weâ€™ve taken the next step to allow Spark to integrate natively with a far larger number of
input sources.  These new integrations are made possible through the inclusion of the new Spark SQL Data Sources API.
%https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html

Integration of data vaults into Spark. Access only to Hive Tables.

Monunt external data files as Data Frames. A DataFrame is a distributed collection of data organized into
named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python,
but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources
such as: structured data files, tables in Hive, external databases, or existing RDDs.

In Spark not possible to make and RDD out of NetCDF.
There are specialized modules for each tool, but the combination of different data sets is not possible.

%Tricks on how to run spark within containers and be scheduled by Keubernet:
%http://www.lshift.net/blog/2015/04/30/swarming-spark/
%http://www.sparkexpert.com/2015/01/02/load-database-data-into-spark-using-jdbcrdd-in-java/
%http://stackoverflow.com/questions/11816609/column-based-or-row-based-for-hbase
